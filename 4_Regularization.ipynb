{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/QrpmVp6zJpMxupaCqgqH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MelihKlc/Deep-Learning/blob/main/4_Regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4pxZGpxu5vc"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "import scipy.io\n",
        "from reg_utils import sigmoid, relu, plot_decision_boundary, initialize_parameters, load_2D_dataset, predict_dec\n",
        "from reg_utils import compute_cost, predict, forward_propagation, backward_propagation, update_parameters\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, train_Y, test_X, test_Y = load_2D_dataset()"
      ],
      "metadata": {
        "id": "KkRCv-iVzsQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):\n",
        "    \"\"\"\n",
        "    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n",
        "\n",
        "    Arguments:\n",
        "    X -- input data, of shape (input size, number of examples)\n",
        "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)\n",
        "    learning_rate -- learning rate of the optimization\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    print_cost -- If True, print the cost every 10000 iterations\n",
        "    lambd -- regularization hyperparameter, scalar\n",
        "    keep_prob - probability of keeping a neuron active during drop-out, scalar.\n",
        "\n",
        "    Returns:\n",
        "    parameters -- parameters learned by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "\n",
        "    grads = {}\n",
        "    costs = []                            # to keep track of the cost\n",
        "    m = X.shape[1]                        # number of examples\n",
        "    layers_dims = [X.shape[0], 20, 3, 1]\n",
        "\n",
        "    # Initialize parameters dictionary.\n",
        "    parameters = initialize_parameters(layers_dims)\n",
        "\n",
        "    # Loop (gradient descent)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
        "        if keep_prob == 1:\n",
        "            a3, cache = forward_propagation(X, parameters)\n",
        "        elif keep_prob < 1:\n",
        "            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n",
        "\n",
        "        # Cost function\n",
        "        if lambd == 0:\n",
        "            cost = compute_cost(a3, Y)\n",
        "        else:\n",
        "            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)\n",
        "\n",
        "        # Backward propagation.\n",
        "        assert (lambd == 0 or keep_prob == 1)   # it is possible to use both L2 regularization and dropout,\n",
        "                                                # but this assignment will only explore one at a time\n",
        "        if lambd == 0 and keep_prob == 1:\n",
        "            grads = backward_propagation(X, Y, cache)\n",
        "        elif lambd != 0:\n",
        "            grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n",
        "        elif keep_prob < 1:\n",
        "            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n",
        "\n",
        "        # Update parameters.\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "        # Print the loss every 10000 iterations\n",
        "        if print_cost and i % 10000 == 0:\n",
        "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
        "        if print_cost and i % 1000 == 0:\n",
        "            costs.append(cost)\n",
        "\n",
        "    # plot the cost\n",
        "    plt.plot(costs)\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (x1,000)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "GVVGuara0DKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = model(train_X, train_Y)\n",
        "print (\"On the training set:\")\n",
        "predictions_train = predict(train_X, train_Y, parameters)\n",
        "print (\"On the test set:\")\n",
        "predictions_test = predict(test_X, test_Y, parameters)"
      ],
      "metadata": {
        "id": "Lj7W5StjyLyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title(\"Model without regularization\")\n",
        "axes = plt.gca()\n",
        "axes.set_xlim([-0.75,0.40])\n",
        "axes.set_ylim([-0.75,0.65])\n",
        "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
      ],
      "metadata": {
        "id": "BXDjYwqiyRpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost_with_regularization(A3, Y, parameters, lambd):\n",
        "    \"\"\"\n",
        "    Implement the cost function with L2 regularization. See formula (2) above.\n",
        "\n",
        "    Arguments:\n",
        "    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)\n",
        "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
        "    parameters -- python dictionary containing parameters of the model\n",
        "\n",
        "    Returns:\n",
        "    cost - value of the regularized loss function (formula (2))\n",
        "    \"\"\"\n",
        "    m = Y.shape[1]\n",
        "    W1 = parameters[\"W1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    W3 = parameters[\"W3\"]\n",
        "\n",
        "    cross_entropy_cost = compute_cost(A3, Y) # This gives you the cross-entropy part of the cost\n",
        "    L2_regularization_cost = (np.sum( np.square(W1) ) + np.sum( np.square(W2) ) + np.sum( np.square(W3) ) ) * (1/m) * (lambd/2)\n",
        "    cost = cross_entropy_cost + L2_regularization_cost\n",
        "\n",
        "    return cost"
      ],
      "metadata": {
        "id": "uhFGdpqDRv6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_propagation_with_regularization(X, Y, cache, lambd):\n",
        "    \"\"\"\n",
        "    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n",
        "\n",
        "    Arguments:\n",
        "    X -- input dataset, of shape (input size, number of examples)\n",
        "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
        "    cache -- cache output from forward_propagation()\n",
        "    lambd -- regularization hyperparameter, scalar\n",
        "\n",
        "    Returns:\n",
        "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
        "    \"\"\"\n",
        "\n",
        "    m = X.shape[1]\n",
        "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
        "    dZ3 = A3 - Y\n",
        "    dW3 = (1./m) * np.dot(dZ3 , A2.T) + (lambd/m) * W3\n",
        "    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n",
        "    dA2 = np.dot(W3.T, dZ3)\n",
        "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
        "    dW2 = (1./m) * np.dot(dZ2 , A1.T) + (lambd/m) * W2\n",
        "    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "a8SicRQ9hMSw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}