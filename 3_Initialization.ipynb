{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMw84Xaa3TH8xS2da9fp9DW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MelihKlc/Deep-Learning/blob/main/3_Initialization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRLeQkn5T8c5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "from init_utils import sigmoid, relu, compute_loss, forward_propagation, backward_propagation\n",
        "from init_utils import update_parameters, predict, load_dataset, plot_decision_boundary, predict_dec\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, train_Y, test_X, test_Y = load_dataset()"
      ],
      "metadata": {
        "id": "D1amkPRUUXS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(X, Y, learning_rate = 0.01, num_iterations = 15000, print_cost = True, initialization = \"he\"):\n",
        "    \"\"\"\n",
        "    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n",
        "\n",
        "    Arguments:\n",
        "    X -- input data, of shape (2, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples)\n",
        "    learning_rate -- learning rate for gradient descent\n",
        "    num_iterations -- number of iterations to run gradient descent\n",
        "    print_cost -- if True, print the cost every 1000 iterations\n",
        "    initialization -- flag to choose which initialization to use (\"zeros\",\"random\" or \"he\")\n",
        "\n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model\n",
        "    \"\"\"\n",
        "\n",
        "    grads = {}\n",
        "    costs = [] # to keep track of the loss\n",
        "    m = X.shape[1] # number of examples\n",
        "    layers_dims = [X.shape[0], 10, 5, 1]\n",
        "\n",
        "    # Initialize parameters dictionary.\n",
        "    if initialization == \"zeros\":\n",
        "        parameters = initialize_parameters_zeros(layers_dims)\n",
        "    elif initialization == \"random\":\n",
        "        parameters = initialize_parameters_random(layers_dims)\n",
        "    elif initialization == \"he\":\n",
        "        parameters = initialize_parameters_he(layers_dims)\n",
        "\n",
        "    # Loop (gradient descent)\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "\n",
        "        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
        "        a3, cache = forward_propagation(X, parameters)\n",
        "\n",
        "        # Loss\n",
        "        cost = compute_loss(a3, Y)\n",
        "\n",
        "        # Backward propagation.\n",
        "        grads = backward_propagation(X, Y, cache)\n",
        "\n",
        "        # Update parameters.\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "        # Print the loss every 1000 iterations\n",
        "        if print_cost and i % 1000 == 0:\n",
        "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
        "            costs.append(cost)\n",
        "\n",
        "    # plot the loss\n",
        "    plt.plot(costs)\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per hundreds)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "xLmqVs_PUXtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters_zeros(layers_dims):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    layer_dims -- python array (list) containing the size of each layer.\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n",
        "                    b1 -- bias vector of shape (layers_dims[1], 1)\n",
        "                    ...\n",
        "                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n",
        "                    bL -- bias vector of shape (layers_dims[L], 1)\n",
        "    \"\"\"\n",
        "\n",
        "    parameters = {}\n",
        "    L = len(layers_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "\n",
        "        parameters['W' + str(l)] = np.zeros(shape = (layers_dims[l] , layers_dims[l-1]))\n",
        "        parameters['b' + str(l)] = np.zeros(shape = (layers_dims[l] , 1))\n",
        "\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "yDskgKcvYYoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = model(train_X, train_Y, initialization = \"zeros\")\n",
        "print (\"On the train set:\")\n",
        "predictions_train = predict(train_X, train_Y, parameters)\n",
        "print (\"On the test set:\")\n",
        "predictions_test = predict(test_X, test_Y, parameters)"
      ],
      "metadata": {
        "id": "5OABUk7lYkFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title(\"Model with Zeros initialization\")\n",
        "axes = plt.gca()\n",
        "axes.set_xlim([-1.5,1.5])\n",
        "axes.set_ylim([-1.5,1.5])\n",
        "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
      ],
      "metadata": {
        "id": "42nUl1w5Ysxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters_random(layers_dims):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    layer_dims -- python array (list) containing the size of each layer.\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n",
        "                    b1 -- bias vector of shape (layers_dims[1], 1)\n",
        "                    ...\n",
        "                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n",
        "                    bL -- bias vector of shape (layers_dims[L], 1)\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(3)               # This seed makes sure your \"random\" numbers will be the as ours\n",
        "    parameters = {}\n",
        "    L = len(layers_dims)            # integer representing the number of layers\n",
        "\n",
        "    for l in range(1, L):\n",
        "\n",
        "        parameters['W' + str(l)] = np.random.randn(layers_dims[l] , layers_dims[l-1]) * 10\n",
        "        parameters['b' + str(l)] = np.zeros(shape = (layers_dims[l] , 1))\n",
        "\n",
        "\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "Wwhy9UsQGmoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = model(train_X, train_Y, initialization = \"random\")\n",
        "print (\"On the train set:\")\n",
        "predictions_train = predict(train_X, train_Y, parameters)\n",
        "print (\"On the test set:\")\n",
        "predictions_test = predict(test_X, test_Y, parameters)"
      ],
      "metadata": {
        "id": "f12yO0tJHCmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title(\"Model with large random initialization\")\n",
        "axes = plt.gca()\n",
        "axes.set_xlim([-1.5,1.5])\n",
        "axes.set_ylim([-1.5,1.5])\n",
        "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
      ],
      "metadata": {
        "id": "wKcxMv68O_iC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters_he(layers_dims):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    layer_dims -- python array (list) containing the size of each layer.\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n",
        "                    b1 -- bias vector of shape (layers_dims[1], 1)\n",
        "                    ...\n",
        "                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n",
        "                    bL -- bias vector of shape (layers_dims[L], 1)\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layers_dims) - 1 # integer representing the number of layers\n",
        "\n",
        "    for l in range(1, L + 1):\n",
        "\n",
        "        parameters['W' + str(l)] = np.random.randn(layers_dims[l] , layers_dims[l-1]) * np.sqrt( 2. / layers_dims[l-1])\n",
        "        parameters['b' + str(l)] = np.zeros(shape = (layers_dims[l] , 1))\n",
        "\n",
        "\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "OdNC_-8UPHsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = model(train_X, train_Y, initialization = \"he\")\n",
        "print (\"On the train set:\")\n",
        "predictions_train = predict(train_X, train_Y, parameters)\n",
        "print (\"On the test set:\")\n",
        "predictions_test = predict(test_X, test_Y, parameters)"
      ],
      "metadata": {
        "id": "dOo8LXa5PcRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title(\"Model with He initialization\")\n",
        "axes = plt.gca()\n",
        "axes.set_xlim([-1.5,1.5])\n",
        "axes.set_ylim([-1.5,1.5])\n",
        "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
      ],
      "metadata": {
        "id": "PHFR3xrPPkV-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}